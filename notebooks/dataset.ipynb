{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "import yaml\n",
    "\n",
    "import requests\n",
    "import yfinance as yf\n",
    "import pandas_datareader.data as web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import CONFIG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = 10\n",
    "dt_start = (dt.datetime.now() - dt.timedelta(days=years*365)).date()\n",
    "dt_end=dt.datetime.now().date()\n",
    "\n",
    "print('Extraindo informações de {} até {}'.format(dt_start, dt_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Caminho do seu arquivo\n",
    "yaml_path = CONFIG_DIR / 'dataset.yaml'\n",
    "\n",
    "# Lê o arquivo YAML\n",
    "with open(yaml_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Baixa os dados do yfinance\n",
    "yf_data = {}\n",
    "for nome, ticker in config['yfinance']['tickers_code'].items():\n",
    "    print(f'Baixando {nome} ({ticker}) via yfinance...')\n",
    "    df = yf.download(ticker, start=start_date, end=end_date)\n",
    "    yf_data[nome] = df'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.DatasetLoader import YfinanceLoader\n",
    "# Create an instance of YfinanceLoader with the start and end dates\n",
    "yf_loader = YfinanceLoader(start_date=str(dt_start), end_date=str(dt_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import pandas_datareader as  pdr\n",
    "\n",
    "# 5Baixa os dados do DataReader (ex: FRED)\n",
    "dr_data = {}\n",
    "for codigo, nome in config['DataReader']['reader_code'].items():\n",
    "    print(f'Baixando {nome} ({codigo}) via DataReader...')\n",
    "    df = pdr.DataReader(codigo, 'fred', start=start_date, end=end_date)\n",
    "    dr_data[nome] = df'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.DatasetLoader import DataReaderLoader\n",
    "\n",
    "# Create an instance of YfinanceLoader with the start and end dates\n",
    "dr_loader = DataReaderLoader(start_date=str(dt_start), end_date=str(dt_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Função para buscar uma série do SGS\n",
    "def get_bcb_series(sgs_code, start,end):\n",
    "    url = f'https://api.bcb.gov.br/dados/serie/bcdata.sgs.{sgs_code}/dados'\n",
    "    \n",
    "    # Monta os parâmetros corretamente no formato da API\n",
    "    params = {\n",
    "        'formato': 'json',\n",
    "        'dataInicial': start.strftime('%d/%m/%Y'),  # Formato dd/mm/yyyy\n",
    "        'dataFinal': end.strftime('%d/%m/%Y'),      # Formato dd/mm/yyyy\n",
    "    }\n",
    "\n",
    "    # Requisição\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    # Verifica se a resposta está vazia\n",
    "    if not data:\n",
    "        print(f\"Warning: No data found for SGS code {sgs_code} between {start} and {end}.\")\n",
    "        return data\n",
    "    \n",
    "    return data'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''bcb_data = {}\n",
    "for nome, codigo in config['bcb']['sgs_code'].items():\n",
    "    print(f'Baixando {nome} ({codigo}) via API bcb...')\n",
    "    df = get_bcb_series(codigo, start=start_date,end=end_date)\n",
    "    bcb_data[nome] = df'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.DatasetLoader import BcbLoader\n",
    "\n",
    "# Create an instance of YfinanceLoader with the start and end dates\n",
    "bcb_loader = BcbLoader(start_date=str(dt_start), end_date=str(dt_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcb_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import RAW_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Opcional) Salva em CSV\n",
    "for nome, df in yf_data.items():\n",
    "    df.to_csv(RAW_DATA_DIR / f'dados_yf_{nome}.csv')\n",
    "\n",
    "for nome, df in bcb_data.items():\n",
    "    df.to_csv(RAW_DATA_DIR / f'dados_bcb_{nome}.csv')\n",
    "\n",
    "for nome, df in dr_data.items():\n",
    "    df.to_csv(RAW_DATA_DIR / f'dados_dr_{nome}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### yfinance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Indicator**        | **Ticker (Yahoo Finance)** | **Description** |\n",
    "|---------------------|--------------------------|-------------|\n",
    "| **IBOVESPA**       | `^BVSP`                   | Brazil Stock Market Index |\n",
    "| **Commodities**     | `GC=F`, `CL=F`, `SB=F`, `ZC=F` | Gold, Crude Oil, Sugar, Corn |\n",
    "| **Stock Market Index (S&P 500)** | `^GSPC` | Standard & Poor’s 500 (S&P 500) Index |\n",
    "| **Cryptocurrency (Bitcoin)** | `BTC-USD` | Bitcoin price in USD |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the IBOVESPA ticker symbol used on Yahoo Finance\n",
    "tickers = [\"^BVSP\",\"^GSPC\",\"BTC-USD\", \"GC=F\", \"CL=F\", \"SB=F\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download historical data (default is daily interval)\n",
    "# You can adjust the period (e.g., '1y', '5y', 'max') or set specific dates\n",
    "df_yf = yf.download(tickers, start=dt_start, end=dt_end).ffill()\n",
    "\n",
    "# Ensure the 'Date' column exists and is in datetime format before setting it as the index\n",
    "if 'Date' in df_yf.columns:\n",
    "    df_yf['Date'] = pd.to_datetime(df_yf['Date'])\n",
    "    df_yf.set_index('Date', inplace=True)\n",
    "else:\n",
    "    print(\"The 'Date' column is not present in the dataset.\")\n",
    "\n",
    "# Display the first few rows\n",
    "df_yf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Exchange Rate (Forex)** | `USDBRL=X`, `EURBRL=X` | USD/BRL (Dollar to Real), EUR/BRL (Euro to Real) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the multi-level column index\n",
    "df_yf.columns = ['_'.join(col).strip() for col in df_yf.columns.values]\n",
    "\n",
    "# Display the first few rows of the updated dataset\n",
    "df_yf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bcb - Banco Central do Brasil\n",
    "\n",
    "https://www3.bcb.gov.br/sgspub/localizarseries/localizarSeries.do?method=prepararTelaLocalizarSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_br = {\n",
    "    'SELIC':11,\n",
    "    'CDI':12,\n",
    "    'SELIC_Anual': 1178,\n",
    "    'SELIC_Meta_Anual': 432,\n",
    "    'IPCA_Mensal': 433,\n",
    "    'IGP_M_Mensal': 189,\n",
    "    'INCC_Mensal': 192,\n",
    "    'Indice_Condicoes_Econ_BR': 27574,\n",
    "    'Indice_Condicoes_Econ_BR_USD': 29042,\n",
    "    'Salario_Minimo': 1619,\n",
    "    'IBC_BR': 24363,\n",
    "    'Populacao_BR': 21774,\n",
    "    'PIB_Trimestral_Real': 4380,\n",
    "    'PIB_Anual_Corrente': 7326,\n",
    "    'Deflator_Implicito_PIB': 1211\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para buscar uma série do SGS\n",
    "def get_bcb_series(sgs_code, start,end):\n",
    "    url = f'https://api.bcb.gov.br/dados/serie/bcdata.sgs.{sgs_code}/dados'\n",
    "    \n",
    "    # Monta os parâmetros corretamente no formato da API\n",
    "    params = {\n",
    "        'formato': 'json',\n",
    "        'dataInicial': start.strftime('%d/%m/%Y'),  # Formato dd/mm/yyyy\n",
    "        'dataFinal': end.strftime('%d/%m/%Y'),      # Formato dd/mm/yyyy\n",
    "    }\n",
    "\n",
    "    # Requisição\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    # Verifica se a resposta está vazia\n",
    "    if not data:\n",
    "        print(f\"Warning: No data found for SGS code {sgs_code} between {start} and {end}.\")\n",
    "        return data\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixar todas as séries e armazenar num dicionário\n",
    "br_dataframes = {}\n",
    "for name, code in series_br.items():\n",
    "    print(f'Baixando {name} (código {code})...')\n",
    "    try:\n",
    "        br_dataframes[name] = pd.DataFrame(get_bcb_series(code, start=dt_start,end=dt_end))\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao baixar a série {name} (código {code}): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all DataFrames in the dictionary into a single DataFrame\n",
    "df_br = pd.concat(\n",
    "    {key: df.assign(data=pd.to_datetime(df['data'], format='%d/%m/%Y'))\n",
    "          .set_index('data')['valor']\n",
    "     for key, df in br_dataframes.items()},\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by index (date)\n",
    "df_br.sort_index(inplace=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df_br.ffill().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas_datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário com os códigos do FRED e nomes mais amigáveis\n",
    "series_usa = {\n",
    "    'DEXBZUS': 'BRL_USD',\n",
    "    'CPIAUCSL': 'CPI_USA',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puxar todas as séries e juntar num único DataFrame\n",
    "df_usa = pd.concat(\n",
    "    [web.DataReader(code, 'fred', dt_start, dt_end).rename(columns={code: name})\n",
    "     for code, name in series_usa.items()],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usa.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Juntar os três DataFrames com base no índice\n",
    "dataset = df_yf.join([df_br, df_usa], how='left')\n",
    "\n",
    "dataset.ffill(inplace=True) # Preencher valores ausentes com o último valor conhecido\n",
    "dataset.bfill(inplace=True) # Preencher valores ausentes com o último valor conhecido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir as primeiras linhas do DataFrame resultante\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir as primeiras linhas do DataFrame resultante\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc['2025-01-02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[:, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a CSV file\n",
    "dataset.to_csv('../data/raw/dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "dataset = pd.read_csv('../data/raw/dataset.csv', index_col=0, parse_dates=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_target = {}\n",
    "for index in dataset.columns[dataset.columns.str.contains('BVSP')]:\n",
    "    dict_target[index] = dataset.columns.get_loc(index)\n",
    "\n",
    "print(dict_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ColumnTransformer usando make_column_selector\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scaler', StandardScaler(), make_column_selector(dtype_include=np.number))\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Pipeline final\n",
    "pipeline_process = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Fit and transform the pipeline on the selected data\n",
    "X_all = pipeline_process.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from scipy.sparse import issparse\n",
    "\n",
    "if issparse(X_all):\n",
    "    X_all = X_all.toarray()\n",
    "\n",
    "X_all.astype(np.float64)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dict_target.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar os pares (X, y) para todo o histórico\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "sequence_length = 200  # Número de dias para prever o fechamento do ibovespa\n",
    "target_column_index = list(dict_target.values())  # o fechamento do ibovespa está no íncide 4 de X_all\n",
    "\n",
    "generator = TimeseriesGenerator(\n",
    "    X_all, X_all[:, target_column_index],\n",
    "    length=sequence_length, batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generator[0][0].shape)\n",
    "print(generator.data.shape)\n",
    "print(generator.targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator[0][0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator[0][0].shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'checkpoint.keras',           # nome do arquivo salvo\n",
    "    save_best_only=False,         # se quiser salvar sempre, não só o melhor\n",
    "    save_weights_only=False,      # se quiser salvar o modelo completo\n",
    "    save_freq='epoch'             # salva a cada época\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(MODELS_DIR / 'checkpoint.keras')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from src.config import MODELS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the checkpoint callback\n",
    "checkpoint = ModelCheckpoint(\n",
    "    MODELS_DIR / 'checkpoint.keras',           # nome do arquivo salvo\n",
    "    save_best_only=False,         # se quiser salvar sempre, não só o melhor\n",
    "    save_weights_only=False,      # se quiser salvar o modelo completo\n",
    "    save_freq='epoch'             # salva a cada época\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Bidirectional, LSTM, Dropout, Dense\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Define o formato de entrada\n",
    "input_shape = (generator[0][0].shape[1], generator[0][0].shape[2])\n",
    "\n",
    "# Define e compila o modelo LSTM\n",
    "model = Sequential([\n",
    "    Input(shape=input_shape),\n",
    "    Bidirectional(LSTM(70, return_sequences=True, recurrent_dropout=0.2, kernel_regularizer=l2(0.001))),\n",
    "    Dropout(0.3),\n",
    "    LSTM(50, return_sequences=True, recurrent_dropout=0.2, kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "    LSTM(30, return_sequences=False, recurrent_dropout=0.2, kernel_regularizer=l2(0.001)),\n",
    "    Dense(5)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(generator, epochs=30,callbacks=[checkpoint,early_stopping],verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Últimos 7 dias de X\n",
    "last_window = X_all[-sequence_length:]  # shape (7, features)\n",
    "last_window = last_window.reshape((1, sequence_length, X_all.shape[1]))  # (1, 7, features)\n",
    "\n",
    "# Previsão do próximo dia\n",
    "next_prediction = model.predict(last_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_prediction[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = pipeline_process.named_steps['preprocessor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ct.transformers_[0][2]  # Pega os nomes/índices das colunas usadas no primeiro transformador\n",
    "scaler = ct.transformers_[0][1]        # Pega o StandardScaler associado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the integer index of the target column in numeric_cols\n",
    "target_in_scaler_index = 4\n",
    "\n",
    "dummy_input = np.zeros((1, len(numeric_cols)))\n",
    "dummy_input[0, target_in_scaler_index] = next_prediction[0, 0]\n",
    "\n",
    "inv = scaler.inverse_transform(dummy_input)\n",
    "next_prediction_real = inv[0, target_in_scaler_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape para 2D: (7, features)\n",
    "last_window_2d = last_window.reshape(-1, X_all.shape[1])\n",
    "\n",
    "# Selecionar apenas as colunas numéricas correspondentes a numeric_cols\n",
    "last_window_2d_numeric = last_window_2d[:, :len(numeric_cols)]\n",
    "\n",
    "# Inverter a transformação\n",
    "last_window_real_numeric = scaler.inverse_transform(last_window_2d_numeric)\n",
    "\n",
    "# Recriar o array completo com os valores invertidos\n",
    "last_window_real = last_window.copy()\n",
    "last_window_real[:, :, :len(numeric_cols)] = last_window_real_numeric\n",
    "\n",
    "# Se quiser, pode voltar ao shape 3D depois\n",
    "last_window_real = last_window_real.reshape(1, 7, X_all.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_window_real_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Eixo X para a janela de entrada (últimos 7 dias)\n",
    "x_input_dates = pd.date_range(end=dt_end, periods=sequence_length).strftime('%Y-%m-%d')\n",
    "\n",
    "# Eixo X para a predição (logo após a janela)\n",
    "x_pred_dates = [x_input_dates[-1], (pd.to_datetime(x_input_dates[-1]) + pd.Timedelta(days=1)).strftime('%Y-%m-%d')]\n",
    "\n",
    "# Último valor real + predição real\n",
    "y_pred_real = [last_window_real[0, -1, target_column_index], next_prediction_real]\n",
    "\n",
    "# Calcular a diferença percentual entre o predito e o último real\n",
    "diff_percent = ((y_pred_real[1] - y_pred_real[0]) / y_pred_real[0]) * 100\n",
    "\n",
    "# Formatar os valores para exibição\n",
    "y_pred_real_formatted = [f\"{int(y):,}\".replace(\",\", \".\") for y in y_pred_real]\n",
    "diff_percent_formatted = f\"{diff_percent:.2f}%\"\n",
    "\n",
    "# Plot da janela de dados reais\n",
    "plt.plot(x_input_dates, last_window_real[0, :, target_column_index], label='Últimos dados reais')\n",
    "\n",
    "# Plot da predição real como linha pontilhada conectando ao último ponto real\n",
    "plt.plot(x_pred_dates, y_pred_real, 'r--', label='Predição real')\n",
    "\n",
    "# Adicionar rótulo ao último ponto real\n",
    "plt.annotate(f'{y_pred_real_formatted[0]}', \n",
    "             (x_input_dates[-1], y_pred_real[0]), \n",
    "             textcoords=\"offset points\", \n",
    "             xytext=(-10, 10), \n",
    "             ha='center', \n",
    "             fontsize=9, \n",
    "             color='blue')\n",
    "\n",
    "# Adicionar rótulo ao ponto predito com a diferença percentual\n",
    "plt.annotate(f'{y_pred_real_formatted[1]} ({diff_percent_formatted})', \n",
    "             (x_pred_dates[-1], y_pred_real[1]), \n",
    "             textcoords=\"offset points\", \n",
    "             xytext=(-10, 10), \n",
    "             ha='center', \n",
    "             fontsize=9, \n",
    "             color='red')\n",
    "\n",
    "# Legenda e rótulos\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valor Real')\n",
    "plt.title('Predição do modelo vs Últimos dados reais')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
