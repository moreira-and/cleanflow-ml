{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff67105",
   "metadata": {},
   "source": [
    "# feature_selection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c92a9",
   "metadata": {},
   "source": [
    "## Introdu√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63ace81",
   "metadata": {},
   "source": [
    "Estruturar o racioc√≠nio:\n",
    "- **(1) Entendimento conceitual profundo:** por que *feature selection* existe, qual o problema que ela resolve e como o `sklearn.feature_selection` se encaixa.\n",
    "- **(2) Estrat√©gias principais de sele√ß√£o de features:** vis√£o geral das abordagens dentro do `sklearn`.\n",
    "- **(3) Casos pr√°ticos t√≠picos:** exemplos reais que motivam o uso.\n",
    "- **(4) Linha de racioc√≠nio pr√°tica:** como um especialista pensa ao escolher m√©todos e interpretar resultados.\n",
    "- **(5) Demonstra√ß√£o com exemplos em c√≥digo.  \n",
    "\n",
    "Vou ser bem t√©cnico e aplicar vis√£o de especialista, mas de forma did√°tica.\n",
    "\n",
    "---\n",
    "\n",
    "## (1) Entendimento conceitual profundo\n",
    "\n",
    "**Por que feature selection √© necess√°rio?**\n",
    "\n",
    "- **Reduzir overfitting:** Features irrelevantes introduzem ru√≠do no modelo ‚Üí aumenta o risco do modelo aprender padr√µes falsos.\n",
    "- **Aumentar a acur√°cia:** Removendo vari√°veis in√∫teis, o modelo foca no que importa.\n",
    "- **Melhorar a interpretabilidade:** Menos vari√°veis = modelos mais compreens√≠veis.\n",
    "- **Reduzir custo computacional:** Especialmente em bases grandes.\n",
    "\n",
    "**Problema real:** Muitos datasets v√™m com vari√°veis que n√£o t√™m rela√ß√£o causal, s√£o redundantes ou apenas carregam ru√≠do.\n",
    "\n",
    "---\n",
    "\n",
    "**Onde a `sklearn.feature_selection` entra?**\n",
    "\n",
    "O `sklearn.feature_selection` fornece m√©todos prontos para:\n",
    "- **Selecionar** vari√°veis √∫teis,\n",
    "- **Rankear** a import√¢ncia das vari√°veis,\n",
    "- **Elimin√°-las** de forma sistem√°tica, com base em estat√≠sticas ou aprendizado de m√°quina.\n",
    "\n",
    "A biblioteca te d√° ferramentas para aplicar estrat√©gias distintas, dependendo do seu objetivo e do tipo de modelo que pretende construir.\n",
    "\n",
    "---\n",
    "\n",
    "## (2) Estrat√©gias principais de sele√ß√£o de features no sklearn\n",
    "\n",
    "Dentro do `sklearn.feature_selection`, temos tr√™s fam√≠lias de m√©todos:\n",
    "\n",
    "| Estrat√©gia             | Como funciona                                    | Exemplos (`sklearn`)                   |\n",
    "|-------------------------|--------------------------------------------------|----------------------------------------|\n",
    "| **Filter methods**       | Avalia cada feature independentemente do modelo | `SelectKBest`, `VarianceThreshold`    |\n",
    "| **Wrapper methods**      | Usa o desempenho do modelo como crit√©rio        | `RFE`, `RFECV`                        |\n",
    "| **Embedded methods**     | Sele√ß√£o √© parte do treinamento do modelo        | `SelectFromModel` com Lasso, √°rvores  |\n",
    "\n",
    "**Resumo:**  \n",
    "- Filter = Pr√©-processamento ‚Üí Estat√≠sticas simples  \n",
    "- Wrapper = Busca guiada ‚Üí Modelos treinados muitas vezes  \n",
    "- Embedded = J√° vem da natureza do modelo ‚Üí Ex: regulariza√ß√£o L1\n",
    "\n",
    "---\n",
    "\n",
    "## (3) Casos pr√°ticos t√≠picos\n",
    "\n",
    "**Exemplo 1: Dataset com muitas vari√°veis categ√≥ricas dummies**\n",
    "\n",
    "- Problema: muitas dummies podem ser irrelevantes e confundir o modelo.\n",
    "- Solu√ß√£o: usar `SelectKBest` com teste de qui-quadrado (`chi2`) para manter apenas as mais relevantes.\n",
    "\n",
    "**Exemplo 2: Dataset num√©rico com alta correla√ß√£o**\n",
    "\n",
    "- Problema: colinearidade entre vari√°veis (multicolinearidade) prejudica modelos lineares.\n",
    "- Solu√ß√£o: usar `VarianceThreshold` para eliminar vari√°veis quase constantes, seguido de an√°lise de correla√ß√£o manual.\n",
    "\n",
    "**Exemplo 3: Queremos otimizar um modelo de classifica√ß√£o**\n",
    "\n",
    "- Problema: alto n√∫mero de vari√°veis gera overfitting.\n",
    "- Solu√ß√£o: usar `RFECV` (Recursive Feature Elimination + valida√ß√£o cruzada) para encontrar o conjunto √≥timo de features.\n",
    "\n",
    "---\n",
    "\n",
    "## (4) Linha de racioc√≠nio pr√°tica de um especialista\n",
    "\n",
    "**Como um especialista pensa sobre feature selection:**\n",
    "\n",
    "1. **Entender o tipo de dados:**\n",
    "   - Vari√°veis categ√≥ricas ou num√©ricas?\n",
    "   - Muitas vari√°veis altamente correlacionadas?\n",
    "   - Muitos valores ausentes?\n",
    "\n",
    "2. **Escolher o m√©todo adequado:**\n",
    "   - **Filter:** se quer uma limpeza r√°pida e preliminar.\n",
    "   - **Wrapper:** se precisa otimizar o desempenho do modelo a qualquer custo computacional.\n",
    "   - **Embedded:** se o modelo naturalmente faz sele√ß√£o (ex: Lasso, √Årvores).\n",
    "\n",
    "3. **Testar diferentes abordagens:**\n",
    "   - N√£o confiar apenas em um m√©todo.\n",
    "   - Comparar os resultados: feature selection pode ser inst√°vel (mudan√ßas dependendo da aleatoriedade).\n",
    "\n",
    "4. **Olhar para o modelo, n√£o s√≥ para os n√∫meros:**\n",
    "   - √Äs vezes manter uma vari√°vel \"fraca\" faz sentido se for f√°cil de explicar para o neg√≥cio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c6d65",
   "metadata": {},
   "source": [
    "## Exemplo 1: Filter Method com SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f5e3d1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Carregar dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Selecionar as 5 melhores features\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Ver quais features foram selecionadas\n",
    "selected_features = selector.get_support(indices=True)\n",
    "print(f\"Selected features indices: {selected_features}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cf1851",
   "metadata": {},
   "source": [
    "Aqui usamos `f_classif`, que √© baseado em ANOVA F-test ‚Äî √≥timo para classifica√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f77b8fe",
   "metadata": {},
   "source": [
    "## Exemplo 2: Wrapper Method com RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d35af2b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Gerar dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=5, random_state=42)\n",
    "\n",
    "# Criar modelo\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Recursive Feature Elimination com valida√ß√£o cruzada\n",
    "rfecv = RFECV(\n",
    "    estimator=model,\n",
    "    step=1,\n",
    "    cv=StratifiedKFold(5),\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "print(f\"Selected features: {rfecv.support_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6259ed",
   "metadata": {},
   "source": [
    "Aqui o algoritmo *elimina recursivamente* a pior feature e treina de novo at√© encontrar o melhor conjunto de vari√°veis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a5713",
   "metadata": {},
   "source": [
    "## Exemplo 3: Embedded Method com SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b7b5bf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Treinar modelo de √°rvore\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Selecionar features importantes\n",
    "sfm = SelectFromModel(model, threshold=\"mean\")\n",
    "X_selected = sfm.transform(X)\n",
    "\n",
    "print(f\"Shape antes: {X.shape}, depois da sele√ß√£o: {X_selected.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90622017",
   "metadata": {},
   "source": [
    "O Random Forest calcula import√¢ncias de features internamente ‚Üí podemos eliminar as menos importantes automaticamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4384758",
   "metadata": {},
   "source": [
    "## Conclus√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4a3fe1",
   "metadata": {},
   "source": [
    "üîµ **Feature selection n√£o √© uma etapa isolada**: ela √© interativa, dependendo do modelo, da m√©trica de avalia√ß√£o e at√© da interpreta√ß√£o para o neg√≥cio.\n",
    "\n",
    "üîµ **Especialistas experimentam, avaliam e interpretam**, n√£o apenas rodam um algoritmo.\n",
    "\n",
    "üîµ **`sklearn.feature_selection` √© poderoso** porque cobre todos os principais tipos de t√©cnicas, prontas para integrar no pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "Se quiser, posso te mostrar ainda:\n",
    "- Como **integrar esses m√©todos em Pipelines do sklearn** (recomendado para projetos s√©rios).\n",
    "- Um **fluxograma de decis√£o** para saber qual m√©todo escolher em qual situa√ß√£o.\n",
    "- Casos mais avan√ßados (ex: L1-based feature selection combinada com Random Forests).\n",
    "\n",
    "Quer que eu siga para algum desses? üöÄ  \n",
    "(Ou se quiser, posso tamb√©m sugerir projetos pr√°ticos para voc√™ treinar isso de verdade.)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
